#APRIORI
import numpy as np
import pandas as pd
from apyori import apriori
 
store_data=  pd.read_csv("file loc", header=None)
print(store_data)
records=[]
for i in range(0,21):
    records.append([str(store_data.values[i,j]) for j in range(0,5)])
    
arule = apriori(records, min_supprt=0.1, min_confidence=0.1, min_lift=1.0, min_length=2)
aresults= list(arule)
 
print(len(aresults))
for i in aresults:
    print(i)
    print("\n")
------------------------------------------------------------


#HEIRARCHICAL CLustering with dataset
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
from sklearn.cluster import AgglomerativeClustering 

data = pd.read_csv('C:\\DATA WAREHOUSING & MINING\\Wholesale customers data.csv') 
data.head() 
 
from sklearn.preprocessing import normalize 
data_scaled = normalize(data) 
data_scaled = pd.DataFrame(data_scaled, columns=data.columns) 
data_scaled.head() 
 
import scipy.cluster.hierarchy as shc 
plt.figure(figsize=(10, 7))   
plt.title("Dendrograms")   
dend = shc.dendrogram(shc.linkage(data_scaled, method='ward')) 
 
from sklearn.cluster import AgglomerativeClustering 
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')   
cluster.fit_predict(data_scaled) 
 
plt.figure(figsize=(10, 7))   
plt.scatter(data_scaled['Milk'], data_scaled['Grocery'], c=cluster.labels_, cmap ='rainbow')  
 
--------------------------------------------------------------------------------

#HEIRARCHICAL CLustering without dataset

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
from sklearn.cluster import AgglomerativeClustering 
  
x=[4,5,10,4,3,11,14,6,10,12] 
y = [21,19,24,17,16,25,24,22,21,21] 
plt.scatter(x,y) 
plt.show() 
  
from scipy.cluster.hierarchy import dendrogram, linkage 
  
data = list(zip(x,y)) 
print(data) 
  
linkage_data = linkage(data, method="ward", metric='euclidean') 
dendrogram(linkage_data) 
  
plt.show() 
  
hierarchical = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage = 'ward') 
labels = hierarchical.fit_predict(data) 
plt.scatter(x,y, c=labels, cmap="rainbow") 
plt.show() 

----------------------------------------------------------------------
2. Decision Tree for Universal Bank dataset
Program:
import pandas as pd
dataset = pd.read_csv("UniversalBank.csv")
x=dataset.iloc[:,[1,2,3,5,6,7,8,9,10,11,12]].values
y=dataset.iloc[:,-1].values
print(dataset.count)
 
from sklearn.model_selection import train_test_split
xtrain, xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.20, random_state=40)
 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
 
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=40,max_depth=3)
classifier.fit(xtrain, ytrain)
 
ypred = classifier.predict(xtest)
 
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import tree
cm = confusion_matrix(ytest, ypred)
ac = accuracy_score(ytest, ypred)
print("Confusion Matrix: ",cm)
print("Accuracy Score: ", ac)
 
tree.plot_tree(classifier)

-------------------------------------------------------------------------1. Decision Tree for Social Network Ads Dataset
Program:
import pandas as pd
dataset = pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:,[2,3]].values
y=dataset.iloc[:,-1].values
print(dataset.count)
 
from sklearn.model_selection import train_test_split
xtrain, xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.25, random_state=0)
 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.transform(xtest)
 
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=3)
classifier.fit(xtrain, ytrain)
 
ypred = classifier.predict(xtest)
 
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn import tree
cm = confusion_matrix(ytest, ypred)
ac = accuracy_score(ytest, ypred)
print("Confusion Matrix: ",cm)
print("Accuracy Score: ", ac)
 
tree.plot_tree(classifier)
--------------------------------------------------------------------------------
2. K Means clustering based on a dataset
Program:
import numpy as nm    
import matplotlib.pyplot as mtp    
import pandas as pd   

# Importing the dataset  
dataset = pd.read_csv('F:\\TYCS 2022-2023\\Mall_Customers.csv')  #D:/AI ML/DWM/Mall_Customers.csv
x = dataset.iloc[:, [3, 4]].values

print(dataset.head())
#finding optimal number of clusters using the elbow method  
from sklearn.cluster import KMeans  
wcss_list= []  #Initializing the list for the values of WCSS  
  
#Using for loop for iterations from 1 to 10.  
for i in range(1, 11):  
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  
    kmeans.fit(x)  
    wcss_list.append(kmeans.inertia_)  
mtp.plot(range(1, 11), wcss_list)  
mtp.title('The Elobw Method Graph')  
mtp.xlabel('Number of clusters(k)')  
mtp.ylabel('wcss_list')  
mtp.show()  

#training the K-means model on a dataset 
kmeans = KMeans(n_clusters=6, init='k-means++', random_state= 42)  
y_predict= kmeans.fit_predict(x)

mtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 20, c = 'blue', label = 'Cluster 1') #for first cluster  
mtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 20, c = 'green', label = 'Cluster 2') #for second cluster  
mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 20, c = 'red', label = 'Cluster 3') #for third cluster  
mtp.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 20, c = 'cyan', label = 'Cluster 4') #for fourth cluster  
mtp.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 20, c = 'magenta', label = 'Cluster 5') #for fifth cluster 
mtp.scatter(x[y_predict == 5, 0], x[y_predict == 5, 1], s = 20, c = 'orange', label = 'Cluster 6') #for sixth cluster  
mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 50, c = 'yellow', label = 'Centroid')   
mtp.title('Clusters of customers')  
mtp.xlabel('Annual Income (k$)')  
mtp.ylabel('Spending Score (1-100)')  
mtp.legend(loc="lower right", fontsize="small")  
mtp.show()
------------------------------------------------------------------------------
 
Hierarchical Clustering with random points 
Input:  
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
from sklearn.cluster import AgglomerativeClustering 
  
x=[4,5,10,4,3,11,14,6,10,12] 
y = [21,19,24,17,16,25,24,22,21,21] 
plt.scatter(x,y) 
plt.show() 
  
from scipy.cluster.hierarchy import dendrogram, linkage 
  
data = list(zip(x,y)) 
print(data) 
  
linkage_data = linkage(data, method="ward", metric='euclidean') 
dendrogram(linkage_data) 
  
plt.show() 
  
hierarchical = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage = 'ward') 
labels = hierarchical.fit_predict(data) 
plt.scatter(x,y, c=labels, cmap="rainbow") 
plt.show() 

-----------------------------------------------------------------------------
 
Hierarchical Clustering with random points 
Input:  
import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd 
from sklearn.cluster import AgglomerativeClustering 
  
x=[4,5,10,4,3,11,14,6,10,12] 
y = [21,19,24,17,16,25,24,22,21,21] 
plt.scatter(x,y) 
plt.show() 
  
from scipy.cluster.hierarchy import dendrogram, linkage 
  
data = list(zip(x,y)) 
print(data) 
  
linkage_data = linkage(data, method="ward", metric='euclidean') 
dendrogram(linkage_data) 
  
plt.show() 
  
hierarchical = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage = 'ward') 
labels = hierarchical.fit_predict(data) 
plt.scatter(x,y, c=labels, cmap="rainbow") 
plt.show() 

